\documentclass[12pt,twoside]{article}
\usepackage{chadstyle}  % Loads my formatting 
\usepackage{longtable,booktabs}
\usepackage{stmaryrd}
%%%%%%%%\usepackage{mdframed}
\usepackage{titlesec}
%\usepackage{hyperref}
%\usepackage[colorlinks,allcolors=blue]{hyperref}
%%%%%%%%%\usepackage{amsthm}
\usepackage{bbm}
\usepackage{ftnxtra}
\usepackage{fnpos}
\usepackage{multirow}
\usepackage{fontawesome}
\usepackage{ulem}

\usepackage{bigints}
%%%%\usepackage[lite]{mtpro2}
\usepackage{physics}
\usepackage{scalerel}
\usepackage{mathrsfs}
\usepackage{empheq}

\usepackage{extarrows}
\usepackage{oubraces}
\usepackage{bm} % bold greak letters

%for algo
\usepackage{algorithm,algorithmic}
%\usepackage[noend]{algpseudocode}
%\algrenewcommand\algorithmicindent{0.5em}%


\usepackage{lmodern}
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\usepackage{csquotes}

%\usepackage{tweaklist}  % I use this package as well; you may need to download

%% Shortcut commands

\usepackage[svgnames]{xcolor}%

\definecolor{maroon}{cmyk}{0, 0.87, 0.68, 0.32}
\definecolor{halfgray}{gray}{0.55}
\definecolor{ipython_frame}{RGB}{207, 207, 207}
\definecolor{ipython_bg}{RGB}{247, 247, 247}
\definecolor{ipython_red}{RGB}{186, 33, 33}
\definecolor{ipython_green}{RGB}{0, 128, 0}
\definecolor{ipython_cyan}{RGB}{64, 128, 128}
\definecolor{ipython_purple}{RGB}{170, 34, 255}


\usepackage[amsmath]{ntheorem}
\usepackage[ntheorem]{mdframed}
\newmdtheoremenv[linewidth=5pt, linecolor=Gainsboro!75!Lavender, topline=false, bottomline=false, skipabove=15pt, skipbelow=20pt]{thm}{\color{ChadBlue}Théorème}

\newmdtheoremenv[linewidth=5pt, linecolor=Gainsboro!75!Lavender, topline=false, bottomline=false, skipabove=15pt, skipbelow=20pt]{lemme}{\color{ChadBlue}Lemme}

\newmdtheoremenv[linewidth=5pt, linecolor=Gainsboro!75!Lavender, topline=false, bottomline=false, skipabove=15pt, skipbelow=20pt]{definition}{\color{ChadBlue}Définition}


%   \theoremclass{Theorem}
%\newtheorem{proposition}{\color{ChadBlue} Théorème}
%\newtheorem{lemme}{\color{ChadBlue} Lemme}
%\newtheorem{definition}{\color{ChadBlue} Définition}

\newcommand{\Proof}[2]{{\hspace{-\parindent} {\color{ChadBlue}\bf Démonstration}~\ref{#1}.}
{#2}\hfill$\blacksquare$ \\ \vspace{.1in}}

\newcommand{\Av}[1] {\underset {#1} {\rm Ave}\,}

\newcommand{\sinc}{\mathrm{sinc}}
\newcommand{\nn}{\nonumber}
%\newcosecnumdepthmmand{\proptitle}[1]{\color{ChadBlue} \textnormal{(#1):}}
%\newtheorem{proposition}{\color{ChadGreen} Proposition}
%\newcommand{\assume}[2]{{\bf{Assumption #1}} (#2)} 
%\newcommand{\clr}[1]{{\color{ChadBlue} #1}}
%\newcommand{\clrg}[1]{{\color{ChadGreen} #1}}
%\newcommand{\Proof}[2]{\newline {\hspace{-\parindent} {\color{ChadGreen}\bf Proof of Proposition}~\ref{#1}.}
%{\color{ChadBlue} #2} \vspace{.1in}}

% If you've loaded *tweaklist.sty* above, uncomment these lines:
% Adjust spacing in itemize/enumerate; see tweaklist.sty
%\renewcommand{\enumhook}{\setlength{\topsep}{2pt}%
%  \setlength{\itemsep}{0pt}}
%\renewcommand{\itemhook}{\setlength{\topsep}{2pt}%
%  \setlength{\itemsep}{0pt}}

\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[shorthands=off,main=french]{babel}
\else
    % See issue https://github.com/reutenauer/polyglossia/issues/127
  \renewcommand*\familydefault{\sfdefault}
    % load polyglossia as late as possible as it *could* call bidi if RTL lang (e.g. Hebrew or Arabic)
  \usepackage{polyglossia}
  \setmainlanguage[]{french}
\fi

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{3}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{0pt}{0pt}
%{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\usepackage{setspace}
\setstretch{1.2}

%eviter les lignes blanches
\raggedbottom

\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}
%\newcommand*\widefbox[1]{{\setlength\fboxsep{10pt}\fbox{\hspace{0.5em}#1\hspace{0.5em}}}}


\newcommand{\sklearn}{\texttt{scikit-learn}}
\newcommand{\itemb}{\item[$\bullet$]}

%JEC
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiterX{\Iintv}[1]{\llbracket}{\rrbracket}{\iintvargs{#1}}
\NewDocumentCommand{\iintvargs}{>{\SplitArgument{1}{,}}m}
{\iintvargsaux#1} %
\NewDocumentCommand{\iintvargsaux}{mm} {#1\mkern1.5mu..\mkern1.5mu#2}


\newcommand{\fcc}[1]{\widehat{#1}^{\raisebox{1pt}{\,\scriptsize$\ast$}}}
\usepackage{stackengine}
\stackMath
\def\hatgap{2pt}
\def\subdown{-2pt}
\newcommand\reallywidehat[2][]{%
\renewcommand\stackalignment{l}%
\stackon[\hatgap]{#2}{%
\stretchto{%
    \scalerel*[\widthof{$#2$}]{\kern-.6pt\bigwedge\kern-.6pt}%
    {\rule[-\textheight/2]{1ex}{\textheight}}%WIDTH-LIMITED BIG WEDGE
}{0.5ex}% THIS SQUEEZES THE WEDGE TO 0.5ex HEIGHT
_{\smash{\belowbaseline[\subdown]{\scriptstyle#1}}}%
}}

\newcommand{\nblink}[1]{\href{https://github.com/jecampagne/ML-toys/blob/main/#1.ipynb}{\faFileCodeO}}


\usepackage{indentfirst}
%\setlength{\parindent}{1.5cm}

\usepackage{listings}


\lstset{
    breaklines=true,
    %
    extendedchars=true,
    literate=
    {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
    {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
    {à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
    {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
    {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
    {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
    {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
    {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
    {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
    {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
    {€}{{\EUR}}1 {£}{{\pounds}}1
}

\lstdefinelanguage{iPython}{
    morekeywords={access,and,break,class,continue,def,del,elif,else,except,exec,finally,for,from,global,if,import,in,is,lambda,not,or,pass,print,raise,return,try,while},%
    %
    % Built-ins
    morekeywords=[2]{abs,all,any,basestring,bin,bool,bytearray,callable,chr,classmethod,cmp,compile,complex,delattr,dict,dir,divmod,enumerate,eval,execfile,file,filter,float,format,frozenset,getattr,globals,hasattr,hash,help,hex,id,input,int,isinstance,issubclass,iter,len,list,locals,long,map,max,memoryview,min,next,object,oct,open,ord,pow,property,range,raw_input,reduce,reload,repr,reversed,round,set,setattr,slice,sorted,staticmethod,str,sum,super,tuple,type,unichr,unicode,vars,xrange,zip,apply,buffer,coerce,intern},%
    %
    sensitive=true,%
    morecomment=[l]\#,%
    morestring=[b]',%
    morestring=[b]",%
    %
    morestring=[s]{'''}{'''},% used for documentation text (mulitiline strings)
    morestring=[s]{"""}{"""},% added by Philipp Matthias Hahn
    %
    morestring=[s]{r'}{'},% `raw' strings
    morestring=[s]{r"}{"},%
    morestring=[s]{r'''}{'''},%
    morestring=[s]{r"""}{"""},%
    morestring=[s]{u'}{'},% unicode strings
    morestring=[s]{u"}{"},%
    morestring=[s]{u'''}{'''},%
    morestring=[s]{u"""}{"""},%
    %
    % {replace}{replacement}{lenght of replace}
    % *{-}{-}{1} will not replace in comments and so on
    literate=
    {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
    {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
    {à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
    {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
    {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
    {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
    {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
    {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
    {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
    {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
    {€}{{\EUR}}1 {£}{{\pounds}}1
    %
    {^}{{{\color{ipython_purple}\^{}}}}1
    {=}{{{\color{ipython_purple}=}}}1
    %
    {+}{{{\color{ipython_purple}+}}}1
    {-}{{{\color{ipython_purple}-}}}1
    {*}{{{\color{ipython_purple}$^\ast$}}}1
    {/}{{{\color{ipython_purple}/}}}1
    %
    {+=}{{{+=}}}1
    {-=}{{{-=}}}1
    {*=}{{{$^\ast$=}}}1
    {/=}{{{/=}}}1,
    literate=
    *{-}{{{\color{ipython_purple}-}}}1
     {?}{{{\color{ipython_purple}?}}}1,
    %
    identifierstyle=\color{black}\ttfamily,
    commentstyle=\color{ipython_cyan}\ttfamily,
    stringstyle=\color{ipython_red}\ttfamily,
    keepspaces=true,
    showspaces=false,
    showstringspaces=false,
    %
    rulecolor=\color{ipython_frame},
    frameround={t}{t}{t}{t},
    numbers=none,
    numberstyle=\tiny\color{halfgray},
    %
    %
    backgroundcolor=\color{ipython_bg},
    %   extendedchars=true,
    %basicstyle=\scriptsize,
    basicstyle=\ttfamily\footnotesize,
    columns=fullflexible,
    keywordstyle=\color{ipython_green}\ttfamily,
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx} % Required for inserting images
\graphicspath{{figures/}}


\title{Statistiques sur générations WCRG et comparaisons avec d'autres modèles}
\author{Jean-Eric Campagne, Etienne Lempereur}
\date{\today}

\begin{document}

\maketitle
\renewcommand{\baselinestretch}{0.75}\normalsize
\tableofcontents
\renewcommand{\baselinestretch}{1.0}\normalsize

\section{Introduction}
Dans cette note est exposée une série de statistiques calculées sur des images générées par une modélisation WCRG décrite dans la référence \cite{2023arXiv230600181G} en les comparant aux mêmes statistiques calculées, non seulement avec les images ayant servies au training du modèle, 
mais aussi avec des images générées par un DGAN, et un modèle "micro-local".
Brièvement, les données utilisées par la suite sont les suivantes: 
\begin{itemize}
\item[WL-1]: il s'agit de cartes de  Weak Lensing\footnote{La référence utilise des résultats de la simulation N-corps GadGet2.} issues d'un modèle Deep GAN (CosmoGAN) de la référence \cite{2019ComAC...6....1M}. L'idée première de l'étude était de comparer les cartes générées par ce DGAN et celles d'un modèle WCRG. Les auteurs de l'article nous ont fourni, pour le training  100,000 images 128x128 pixels, tout comme un lot d'images générées par le réseau CosmoGAN. Ainsi, nous n'avons donc pas ré-entraîné un DGAN à cette occasion. Nous avons extrait 5,000 images\footnote{C'est la statistique utilisée dans la référence \citep{2023arXiv230600181G}} pour l'entraînement d'un modèle WCRG (noté par la suite WCRG-WL1). Nous détaillons un peu plus le traitement dans la section \ref{sec:WL1}.
\item[WL-2 \& $\phi_4$]:  A la suite de la comparaison entre le modèle WCRG-WL1 et le DGAN, des questions se sont posées.  Nous avons donc repris les images d'entraînement et synthétisées des modèles de la référence \cite{2023arXiv230600181G} pour calculer les statistiques que ce soit pour des images de type WL ou de type $\phi_4$ avec différentes valeurs de $\beta=\{0.5, 0.68, 0.76\}$. Cela est détaillé dans la section \ref{sec:WL2_Phi4}
\end{itemize}
\hfill

Les statistiques utilisées sont principalement issues du code python \textit{scattering\_transform} développé par Siho Cheng\footnote{\url{https://github.com/SihaoCheng/scattering_transform} commit Fri Jun 2 11:14:53}
et utilisé entres autres dans les articles \cite{2021arXiv211201288C, 2023arXiv230617210C}. Ces statistiques concernent, les power, bi et tri spectra ainsi que les coefficients de corrélation $C_{00}$ et $C_{01}$. Concernant les données de type WL, la statistique du "comptage de pics" a également été utilisée. Ces statistiques sont décrites dans la section \ref{sec-summary-stat} et les résultats de leurs calculs avec les différents lots de données sont présentés dans les sections \ref{sec-WL1-res} et  \ref{sec:WL2_Phi4}.

Concernant la modélisation WCRG, le code python utilisé est celui du repository mentionné dans la publication afférente\footnote{\url{https://github.com/Elempereur/WCRG/}}.
%
\section{Modélisation WCRG sur les données WL-1}
\label{sec:WL1}
%
\subsection{Phase d'apprentissage}
\label{sec-wcrg-WL1-learning}
%
Avant l'entraînement du modèle WCRG, un traitement des données que l'on peut qualifier de "gaussianisation" a été effectué. En effet, sur la figure \ref{fig-WL1-non-trans}, on peut remarquer que la distribution des valeurs des pixels est mono-mode et peut être transformée en distribution normale par la méthode Cox-Box suivie d'une standardisation classique\footnote{Si $x$ est la valeur d'un pixel original alors primo par l'optimisation Cox-Box on trouve $\lambda^\ast \approx -0.22$, et $x^\prime = (x^{\lambda^\ast}-1)/\lambda^\ast$ suit une loi gaussienne, puis la standardisation classique permet d'obtenir une distribution normale $x^\prime \sim \mathcal{N}(0,1)$.}. Le résultat de la transformation globale est illustré sur la figure \ref{fig-WL1-trans}. Cette transformation globale (inversible) est appliquée sur tous les lots de d'images (entraînement et génération) du DGAN.
%
\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{fig-WL1-img-non-transformed.png}
\includegraphics[width=0.45\textwidth]{fig-WL1-pixelval-non-trans.png}
\caption{Exemple d'une image du lot WL-1 (gauche) et la distribution afférente des valeurs de pixels (droite).}
\label{fig-WL1-non-trans}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{fig-WL1-pixelval-trans.png}
\caption{Distribution des valeurs de pixels de la figure \ref{fig-WL1-non-trans} après application d'une transformation Cox-Box de paramètre $\lambda \approx -0.22$ et d'une standardisation classique.}
\label{fig-WL1-trans}
\end{figure}

Concernant l'optimisation du modèle WCRG\footnote{Notons que l'optimisation du modèle pourrait sans doute se passer de l'opération de gaussianisation préalable, mais l'idée était que cela pourrait aider \textit{a priori} de ne pas avoir à traiter des queues de distributions trop asymétriques.
D'un point de vue pratique, les queues de loi demandent un fine tunning fin des positions et largeurs des potentiels scalaires, qui ne sont pas très robustes au changement de queue (Il faut ajuster les potentiels scalaire selon les quantiles de la queue). Gausianiser permet quasiment d'utiliser des potentiels uniformément positionnés et de moins fine-tuner.}, la démarche est très proche de celle documenté sur le repository du code, en particulier le notebook \textit{Learning\_Models.ipynb} moyennant quelques adaptations décrites ci-après\footnote{Nb. Même si la suite pourrait paraître fastidieuse, une certaine exhaustivité s'impose néanmoins, surtout que les codes n'ont pu être tournés sur notebooks partageables}.
\begin{itemize}
\itemb Les ondelettes utilisées sont de la familles Debauchies d'ordre 4 avec un padding périodique.
\itemb Pour l'ansatz $E(x_J)$ ($J=7$, $L=1$) a été utilisée la fonction 
\begin{lstlisting}[language=iPython]
def ANSATZ_NoCondi(L,centers,sigma,shifts,shifts_sym = False):
    """Non conditionnal ansatz for direct estimation of energy, a scalar potential (sigmoids) + a quadratic potential
    
    Parameters:
    L (int): system size = L*L 
    centers (tensor): position of the centers of the sigmoids 
    sigma (tensor) : width of the sigmoids
    shifts (list of tuples) : spatial shifts for the quadratic potential, carefull (0,0) is already taken into account, do not add here  
    shifts_sym (Bool) : if True, the shifts are not symetrized
    """
\end{lstlisting}
avec 20 sigmoïdes  régulièrement espacées entre le min et le max de la distribution des pixels $\approx [-100,100]$, et par ailleurs \textsf{shifts = ()}). L'optimisation SGD est menée après avoir normalisé le hessien.
%\begin{figure}[h]
%\centering
%\includegraphics[width=0.4\textwidth]{fig-WL1-L1-sigmoids.png}
%\includegraphics[width=0.4\textwidth]{fig-WL1-L1-potentiel.png}
%\caption{Scale $L=1$: sigmoïdes et potentiel optimisé.}
%\label{fig-WL1-L1}
%\end{figure}

\itemb A toutes les échelles $L>1$, nous avons utilisé la fonction 
\begin{lstlisting}[language=iPython]
def ANSATZ_Wavelet(W,L,centers,sigma,mode,shifts,shifts_sym = False):
""" Conditionnal ansatz for conditonal Energy \bar E(\bar x_j\vert x{j}) estimation with a wavelet transform,with a scalar potential (sigmoids) + a quadratic potential
    
    Parameters:
    W (Wavelet) : Wavelet to perfom fast wavelet transform
    L (int): system size ( of x_{j-1}) = L*L 
    centers (tensor): position of the centers of the sigmoids 
    sigma (tensor) : width of the sigmoids
    shifts (list of tuples) : spatial shifts for the quadratic potential, carefull (0,0) is already taken into account, do not add here 
    shifts_sym (Bool) : if True, the shifts are not symetrized
"""    
\end{lstlisting}
Tout comme pour $E_J$, l'optimisation SGD est menée après avoir normalisé le hessien.
%
\itemb A l'échelle $L=2$, 30 sigmoïdes régulièrement espacées dans l'intervalle $[-75, 75]$ sont utilisées, tandis que \textsf{extend = 1.0}. Les autres paramètres sont \textsf{mode = 'All'} et \textsf{shifts = ()}.  
%\begin{figure}[h]
%\centering
%\includegraphics[width=0.4\textwidth]{fig-WL1-L2-sigmoids.png}
%\includegraphics[width=0.4\textwidth]{fig-WL1-L2-potentiel.png}
%\caption{Scale $L=2$: même légende que la figure \ref{fig-WL1-L1}.}
%\label{fig-WL1-L2}
%\end{figure}

%
\itemb A l'échelle $L=4$,  30 sigmoïdes sont également utilisées mais cette fois réparties uniformément selon les quantiles avec $q_{min}=10^{-5}$, $q_{max}=1.0$ et \textsf{extend = 1.0}. De plus \textsf{mode = 'All'} tandis que \textsf{shifts = ((1,0),(0,1),(1,1))} conjointement à \textsf{shifts\_sym = True}.
%
\itemb A l'échelle $L=8$, on utilise 40 sigmoïdes réparties uniformément selon les quantiles avec $q_{min}=10^{-5}$, $q_{max}=1.0$ et \textsf{extend = 1.0}. Par ailleurs outre \textsf{mode = 'All'}, nous avons utilisé la fonction suivante pour définir les shifts tels que 
\textsf{shifts =shift\_modif(L//4)}.
\begin{lstlisting}[language=iPython]
def shift_modif(n):
    shifts =[]
    for i in range(-n,n):
        for j in range(-n,n):
            if i==0 and j==0:
                pass
            else:
                shifts.append((i,j))
    return shifts
\end{lstlisting}
%
\itemb A l'échelle $L=16$, se sont 40 sigmoïdes uniformément réparties sur l'intervalle $[-30, 30]$ que l'on utilise avec \textsf{extend = 1.0}. Par ailleurs \textsf{mode = 'All'} et 
\textsf{shifts =shift\_modif(L//4)}.
%
\itemb A l'échelle $L=32$, on utilise comme précédemment 40 sigmoïdes uniformément réparties sur l'intervalle $[-20, 20]$ (\textsf{extend = 1.0}). Les paramètres \textsf{extend}, \textsf{mode} et \textsf{shifts} ont les mêmes valeurs qu'à l'échelle précédente.
%
\itemb A l'échelle $L=64$, on utilise 30 sigmoïdes réparties uniformément selon les quantiles avec $q_{min}=10^{-5}$, $q_{max}=1.0$ et \textsf{extend = 1.0}. De plus si \textsf{mode = 'All'} comme précédemment, par contre \textsf{shifts = shift\_modif(L//8)}.
%
\itemb Enfin à l'échelle $L=128$ (taille des images d'entraînement), on utilise  30 sigmoïdes uniformément réparties sur l'intervalle $[-7.0, 5.5]$ (\textsf{extend = 0.25}), et si \textsf{mode = 'All'} comme précédemment, par contre \textsf{shifts = shift\_modif(L//16)}.
%
\itemb Les paramètres qui sont particulièrement délicats affectant la synthèse des images aux différentes échelles, sont ceux qui définissent les centres des sigmoïdes (l'intervalle $[min,max]$ pour \textsf{linspace\_centers}, ou $[q_{min},q_{max}]$ pour \textsf{quantile\_centers}. La méthode \textsf{shift\_modif} a été utilisée in fine mais ne change pas vraiment les résultats par rapport à l'usage de \textsf{shift\_quad\_Sym} utilisée de prime abord. Les optimisations SGD sont souvent reprises pour un peu tuner les paramètres tels que le nombre d'époques \textsf{num\_epochs} et le learning rate \text{lr}. Parfois deux étapes d'optimisation ont été pratiquées, notamment avec une valeur de  \text{lr} plus petite pour la seconde étape.
\end{itemize}
%
En pratique, le conditionnement de la loss de l'apprentissage dépend des potentiels scalaires. Ainsi, si ces potentiels ne se superposent pas trop le conditionnement est bon. En revanche, s'ils sont trop redondants le conditionnement diverge. Il faut alors être patient sur l'optimisation et diminuer le learning rate progressivement. 

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{fig-WL1-potentiels-optim.png}
\caption{Potentiels scalaires équivalents $\bar{v}_j$ appris à chaque échelle $j$ pour les images de WL-1.}
\label{fig-WL1-potentiels-optim}
\end{figure}

Concernant les paramètres qui définissent les sigmoïdes à toutes les échelles, ils ont été ajustés à la main largement pour que les histogrammes de contrôles lors de la phase de synthèse (voir la section suivante) montrent une bonne adéquation entre images d'origines et images de synthèse. La technique de sampling est en tout point identique à celle mise en œuvre pour l'article. Ce qui peut changer concerne le nombre de steps et la taille de chaque step.  Notons que dans le cas d'une invariance parfaite d'échelle comme pour $\phi_4$ au point critique, les hyper-paramètres sont les mêmes à chaque échelle (à normalisation par l'écart type de $x_j$ près). Sans auto-similarité, il faut manuellement ajuster. On pourrait utiliser des règles ad-hoc satisfaisant pour fixer les hyperparamètres.

Une fois les paramètres "optimisés" (par ex. les sigmoïdes), les différents potentiels appris $\bar{v}_j(x_{j-1})$ sont présentés sur la figure \ref{fig-WL1-potentiels-optim}.

%
\subsection{Phase de synthèse}
\label{sec-wcrg-WL1-synt}
%
Une fois le modèle appris, on peut regarder à chaque échelle la qualité des synthèses. Les histrogrammes décrits ci-après sont dans l'appendix pour ne pas surchager le corps du texte.  

A l'échelle $L=1$, la comparaison des distributions de valeurs de pixels entre données d'entraînement et synthétisées est montrée sur la figure \ref{fig-WL1-synt-L1-pixelval}. Aux échelles suivantes, outre la comparaison des distributions des valeurs de pixels des images originales et synthétisées, on peut également questionner la distribution des cartes de détails hautes fréquences (horizontales/verticales/diagonales) obtenues par décomposition en ondelettes.
Les résultats aux échelles $L\in[1,128]$ sont présentés sur les figures de \ref{fig-WL1-synt-L2} à \ref{fig-WL1-synt-L128}. Pour des raisons tenant à la taille mémoire des GPUs utilisés, le nombre de cartes générées jusqu'à l'échelle 16 incluse est de 5000, tandis que pour les échelles supérieures on ne peut disposer que de 500 cartes.

A l'échelle $L=128$, on dispose de cartes synthétisées "finales" qui si tout se passe au mieux doivent ressembler comme deux gouttes d'eau aux cartes utilisées pour l'entraînement (ou cartes "originales"). La figure \ref{fig-WL1-synt-exemples} donne quelques exemples de cartes originales et synthétisées.
%
\begin{figure}
\centering
\includegraphics[width=0.99\textwidth]{fig-WL1-synt-exemples.png}
\caption{Exemples de cartes WL-1 utilisées pour l'entraînement et synthétisées une fois le modèles WCRG optimisé.}
\label{fig-WL1-synt-exemples}
\end{figure}
% 
Il est indéniable que le modèle a appris quelque chose. Maintenant, dans l'article \cite{2023arXiv230600181G}, il est montré non seulement des cartes synthétisées, la distribution des valeurs de pixels\footnote{nb. le modèle de l'article a été optimisé directement sur les cartes de WL-2 sans transformation préalable donc on ne peut comparer les modèles. Cela sera sans doute à revoir ultérieurement.} mais aussi la comparaison  du spectre de puissance. Ce dernier point a motivé l'usage de statistiques afin de caractériser les distributions des pixels et de comparer ces statistiques obtenues sur les cartes originales et les synthétisées. Ceci est présenté dans la section suivante.
%
\section{Réduction d'information par l'usage de statistiques}
\label{sec-summary-stat}
%
Un certain nombre de statistiques ont été utilisées afin d'aller au-delà de l'aspect visuel jugeant de la qualité de la synthèse des cartes de WL-1. Comme mentionné dans l'introduction, en premier lieu il a été utilisé le code développé par Siho Cheng afin de calculer:
\begin{itemize}
\itemb les classiques spectre de puissance, \textit{bi}-spectre et \textit{tri}-spectre; 
\itemb le coefficient de scattering $S1_{iso}$, ainsi que les coefficients composites $s_{21}$ (sparsity) et $s_{22}$ (shape) définis dans la référence \cite{2021arXiv211201288C}; 
\itemb et les corrélations de scattering définies selon\footnote{$\langle x \rangle =\Av{u}(x(u))$. Considérant les échelles des ondelettes $j_1\leq j_2\leq j_3$.}
\begin{eqnarray}
 C_{01} &=& \langle(x \ast \psi_2)(|x \ast \psi_1| \ast \psi_2)^\ast\rangle / factor \\
 C_{11} &=& \langle(|x \ast \psi_1| * \psi_3)(|x * \psi_2| * \psi_3)^\ast\rangle / factor
\end{eqnarray}
($C_{01}$ et  $C_{11}$ sont des approximations quadratiques respectivement du bi-spectre et du tri-spectre \citep{2023arXiv230617210C})
avec $x$ la carte de champ considéré (WL, $\phi^4$) de training ou généré, $\psi_i$ des ondelettes de Morlet à différentes échelles et orientations et où le choix de normalisation est défini par
\begin{equation}
factor = L2(x \ast \psi_1) \times L2(x \ast \psi_2)
\end{equation}
Les définitions de ces facteurs sont décrites dans l'article \cite{2023arXiv230617210C}: $C_{01}$ et $C_{11}$ correspondent respectivement à $\tilde{S}_3$ et $\tilde{S}_4$, et la version "isotropique" prise par défaut opère une réduction sur les paires d'angles\footnote{Nb. nous avons vérifié que les effets que nous avons constatés et retranscrits ci-après sont également mis en évidence avec les versions \textit{non-isotropiques}.}. 
\end{itemize}
\hfill

Un point à noter concerne le type des ondelettes utilisées pour le modèle WCRG et pour le calcul des statistiques des images générées. Les représentations $(\bar x_i,x_J)$ et $(x*\psi_i,x*\phi_J)$ sont bien deux représentations en ondelettes, qui sélectionnent des bandes fréquences localisées spatialements, mais la première est réelle, orthogonale et utilise des ondelettes de Daubechies \cite{STEPHANE2009263}, la seconde est redondante, complexe et utilise des Morlets \cite{Meyer}. Ainsi la première représentation est sous-échantillonée, la seconde non. 

L'information contenue dans la phase de $x*\psi_j$ est une information spatiale \cite{dualtreewavelets}. Schématiquement, pour un $\bar x_j$ cosinus, $x*\psi_j$ sera une exponentielle complexe de même période. L'information contenue dans l'amplitude de $\bar x_j$ est dans la phase de $x*\psi_j$.


On pourra faire le parallèle entre $\bar x_i$ et $\text{Re}(x*\psi_i)$ et $x_J$ et $x*\phi_J$, ces cartes couvrant les mêmes bandes fréquences avec une localisation spatiale similaire.
Dans ce cas, le modèle WCRG impose quasiment la covariance de $\text{Re}(x*\psi_i)$ et les histogrammes de $x*\phi_j$.


En pratique afin de fixer un peu les idées, l'obtention des différents spectres et coefficients  s'effectue schématiquement selon les étapes suivantes (si "maps" correspond à un lot de cartes):
\begin{lstlisting}[language=iPython]
bins_pw = 30
Nmaps,M,N = maps.shape
J = int(np.log2(min(M,N))) - 1
pw, kr  = scattering.get_power_spectrum(maps,
              k_range=np.logspace(0,np.log10(M/2*1.4), 
              bins_pw+1))
bi_calc = scattering.Bispectrum_Calculator(M,N, 
              k_range=np.logspace(0,np.log10(M/2*1.4), J-1))
tri_calc = scattering.Trispectrum_Calculator(M,N,
              k_range=np.logspace(0,np.log10(M/2*1.4), J-1))

st_calc = scattering.Scattering2d(M, N, J=6, L=4)

coeffs = st_calc.scattering_coef(maps)
s1  = np.log10(coeffs['S1_iso'])
s21 = np.log10(coeffs['s21'])
s22 = np.log10(coeffs['s22'])

cov_coef = st_calc.scattering_cov(maps)
select_and_index = get_scattering_index(J, L, normalization='P00', C11_criteria='j2>=j1')
c01 = cov_coef['C01_iso'][:,select_and_index['select_2_iso']]
c11 = cov_coef['C11_iso'][:,select_and_index['select_3_iso']]
\end{lstlisting}
De plus, afin de satisfaire des contraintes de taille de mémoire, les spectres/coefficients ont été calculés sur des lots de cartes dont on en a tiré des moyennes et écarts-types (barres d'erreur dans les histogrammes).

Dans le cas des cartes de WL, on a également utilisé le comptage de "pics" au dessus d'un seuil. Il s'agit d'une statistique d'ordre supérieur utilisée pour la recherche de non-gaussianitée qui s'est répandue récemment (voir la référence \cite{2023arXiv230507531L} et les références incluses). Pour se faire la méthode détaillée dans l'article a été utilisée\footnote{\url{https://github.com/LSSTDESC/DifferentiableHOS.git}} avec: à la base la fonction \textsf{find\_peaks2d\_tf} du package \textsf{lenspack}\footnote{\url{https://github.com/CosmoStat/lenspack}} qui cherche la localisation des pics ainsi que leurs "hauteurs", suivie d'un lissage par un noyau gaussien. Le résultat est donc un histogramme de la distribution smoothée des hauteurs de pics d'une carte (ou d'un lot de cartes). Un exemple de recherche de pics est donné sur la figure \ref{fig-WL-peakcount-thr2-exemple}.
\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{fig-WL-peakcount-thr2-exemple.png}
\caption{Exemples de la recherche de pics au dessus d'un seuil ici pris à 2.}
\label{fig-WL-peakcount-thr2-exemple}
\end{figure}
%
\section{Résultats concernant les données WL-1}
\label{sec-WL1-res}
%
Une fois le modèle WCRG obtenu selon la méthode détaillée à la section \ref{sec-wcrg-WL1-learning}, on peut calculer les statistiques décrites dans la section précédente non seulement sur des cartes générées (Sec.~\ref{sec-wcrg-WL1-synt}) mais aussi sur des cartes ayant servi à l'apprentissage du modèle. Les résultats sont présentés sur la figure \ref{fig-WL1-summary-stat} sous les appellations "wcrg" (rouge) et "train" (bleu) \textit{(nb. les autres modèles mentionnés dans l'introduction le DGAN et le "micro-canonique" sont également présentés par anticipation)}. Il est assez remarquable que la quasi totalité des distributions issues de la génération de cartes par le modèle WCRG soient bien, voire très bien reproduites. 
\begin{figure}
\centering
\includegraphics[width=0.99\textwidth]{fig-WL1-summary-stat.png}\\
\includegraphics[width=0.4\textwidth]{fig-WL1-peak-count.png}
\caption{Histogrammes des statistiques détaillées à la section \ref{sec-summary-stat}. Il est a noté qu'excepté le comptage de pics, les distributions sont calculées avec 500 cartes.}
\label{fig-WL1-summary-stat}
\end{figure}
Donc, il n'est pas étonnant que les différences que nous pointons par la suite n'aient pas été mises en évidence dans l'article \cite{2023arXiv230600181G}. 

Le modèle WCRG contraint de façon \textbf{macro}-canonique le spectre, et des histogrammes à chaque échelle. Les statistiques utilisées pour la comparaison n'interviennent donc pas directement dans la modélisation, mais sont vraisemblablement incluses dans les potentiels scalaires. Ceux-ci mélangent non-linéairement les pixels, via les termes $x_j$, qui peuvent être vues comme des moyennes sur des voisinages locaux de tailles $2^j$.

Si on se penche sur les distributions qui dénotent, on constate que:
\begin{itemize}
\itemb en premier lieu ce qui questionne concerne la distribution $C_{01}$ qui manifestement a un problème, alors que la distribution $C_{11}$ est bien reproduite;
\itemb dans une moindre mesure on constate aussi que le bi-spectre (resp. $s_{21}$) est moins bien reproduit que le tri-spectre (resp. $s_{22}$).
\end{itemize} 
Ce sont ces deux points dont l'origine peut être commune qui a motivé les investigations des sections suivantes. 

Notons que si l'on prend les cartes  issues de la génération du réseau CosmoGAN mentionné en introduction, il y a un parfait accord de toutes les distributions "dgan" (orange) avec les distribution d'entraînement ("train") (Fig.~\ref{fig-WL1-summary-stat}). En ce sens, le GAN fait mieux sur ce critère\footnote{Rappel: à l'origine de l'étude était la comparaison "dgan", "wcrg", et dans ce contexte le nombre d'images ayant servit à l'entraînement du modère WCRG est de 5,000 images alors que le DGAN en a utilisé 200,000 \citep{2019ComAC...6....1M}}.  

Nous avons également procédé à la synthèses de cartes en utilisant une modélisation \textbf{micro}-canonique \citep{2023arXiv230617210C} à l'aide du code de Siho Cheng selon typiquement 
\begin{lstlisting}[language=iPython]
 st_calc.scattering_cov(...)['for_synthesis']
\end{lstlisting}
qui utilise les paramètres: le rapport $\langle x\rangle/\sigma(x)$, $L2(x\ast \psi_i)$, $S_1$, ainsi que les parties réelles et imaginaires des coefficients de corrélation $C_{01}$ et $C_{01}$.  Ce type de modélisation/synthèse notée "micro"(vert) sur la figure \ref{fig-WL1-summary-stat} reproduit très bien la quasi totalité des distributions exceptée\footnote{Si l'on ne porte que les distributions "train" et "micro", la remontée du spectre de puissance "micro" est bien visible.} le spectre de puissance à grande valeur de $k$ même si on note que les fluctuations des valeurs sont bien plus réduites que pour les autres modèles pourtant calculées avec le même nombre de cartes. Ceci dit, concernant les coefficients $C_{01}$ l'accord est parfait, mais on s'en doutait puisqu'ils rentrent dans la liste des paramètres ayant servi pour définir la métrique à minimiser afin de produire le modèle micro-canonique.

Dans les sections suivantes, nous allons utiliser les cartes générées par les modèles WCRG ainsi que celles d'entraînement de type WL-2 et $\phi_4$ décrites dans l'introduction, afin de savoir si ces modèles ayant été entraînés dans des conditions différentes performent mieux ou pas que celui envisagé dans cette section avec les données WL-1. 
%
\section{Modèles WCRG et données WL-2 et $\phi_4$}
\label{sec:WL2_Phi4}
%
A la suite des résultats exposés à la section précédente, la question qui vient est de savoir pour quelles raisons la distribution $C_{01}$ (et aussi bi-spectre et $s_{21}$) est nettement moins bien reproduite par le modèle WCRG optimisé comme décrit à la section \ref{sec-wcrg-WL1-learning}. Avant d'esquisser quelques remarques, nous avons utilisé directement (c'est-à-dire sans nouveau entraînement) les modèles et les données de l'article \cite{2023arXiv230600181G} afin d'en calculer les statistiques de la section \ref{sec-summary-stat}.

Concernant les données WL-2, on dispose de 500 cartes d'entraînement ("train") et de 209 cartes générées ("wcrg"). Une note technique: nous avons procéder à un recalage de la distribution des valeurs de pixels des cartes générées afin que la moyenne soit égale à celle des cartes d'entraînement, mais nous n'avons pas opéré de gaussianisation de la section \ref{sec-wcrg-WL1-learning}). Les résultats sont présentés sur la figure \ref{fig-WL2-summary-stat}.
\begin{figure}
\centering
\includegraphics[width=0.99\textwidth]{fig-WL2-summary-stat.png}\\
\includegraphics[width=0.4\textwidth]{fig-WL2-peak-count.png}
\caption{Statistiques de la figure \ref{fig-WL1-summary-stat} mais concernant les données WL-2.}
\label{fig-WL2-summary-stat}
\end{figure}

Par rapport à la figure \ref{fig-WL1-summary-stat} utilisant un modèle optimisé sur les données WL-1 (gaussianisée), on constate: 
\begin{itemize}
\itemb primo que la distribution des $C_{01}$  calculées avec les cartes générées ("wcrg") suit les ondulations de la distribution calculée sur les données d'entraînement ("train"). Mais il y a un petit désaccord néanmoins mais bien plus petit.
\itemb les petits désaccords sur les distribution bi-spectre et $s_{21}$ sont de même ampleur.
\itemb on note également un petit désaccord  sur la distribution des $C_{11}$.
\end{itemize}

Ces données nous renseigne sur le fait qu'un modèle WCRG est capable de générer des cartes avec des coefficients $C_{01}$ convenables en tous les cas bien plus que ceux de la figure \ref{fig-WL1-summary-stat}. 

Concernant les données $\phi_4$, on dispose de 3 lots obtenus avec les valeurs de $\beta$ différentes (ie. $0.5,\ 0.68,\ 0.76$) qui influencent la dynamique sous-jacente. Tout comme pour les cartes WL-2, on a procédé à l'accord des moyennes des distributions des valeurs de pixels entre les cartes générées et les  cartes d'entraînement. Les différents résultats sont exposés sur les figures \ref{fig-phi4-b050-summary-stat}, \ref{fig-phi4-b068-summary-stat}, et \ref{fig-phi4-b076-summary-stat}.

\begin{figure}
\centering
\includegraphics[width=0.99\textwidth]{fig-phi4-b050-summary-stat.png}
\caption{Statistiques de la figure \ref{fig-WL1-summary-stat} mais concernant les données $\phi_4$ ($\beta=0.50$).  Notons que l'on dispose de 500 cartes "train" et de 400 cartes "wcrg".}
\label{fig-phi4-b050-summary-stat}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.99\textwidth]{fig-phi4-b068-summary-stat.png}
\caption{Statistiques de la figure \ref{fig-WL1-summary-stat} mais concernant les données $\phi_4$ ($\beta=0.68$). Notons que l'on dispose de 500 cartes "train" et de 500 cartes "wcrg".}
\label{fig-phi4-b068-summary-stat}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.99\textwidth]{fig-phi4-b076-summary-stat.png}
\caption{Statistiques de la figure \ref{fig-WL1-summary-stat} mais concernant les données $\phi_4$ ($\beta=0.76$). Notons que l'on dispose de 500 cartes "train" et de 50 cartes "wcrg".}
\label{fig-phi4-b076-summary-stat}
\end{figure}

Ce que l'on constate c'est que:
\begin{itemize}
\itemb les distributions obtenues avec les données $\beta=0.50$ (\textit{haute température}) sont remarquablement concordantes; de plus les coefficients $C_{01}$ sont quasi-nuls même sur les données d'entraînement.
\itemb les désaccords entre les distributions avec les données $\beta=0.76$ (\textit{basse température}) sont assez semblables à ceux observés sur les données WL-2.
\itemb la figure des distributions des coefficients $C_{01}$ avec les données $\beta=0.68$ (\textit{point critique}) est particulière: les coefficients calculés avec les données générées sont quasi-nuls, or on constate une structuration de la distribution calculée avec les données d'entraînement. Notons que contrairement au cas WL la distribution du bi-spectre semble bien reproduite (aux erreurs statistiques près), et il en va de même avec la distribution $s_{21}$.
\end{itemize}
Ainsi, on dispose de plusieurs cas de figures et à part 1 cas ($\phi_4$, $\beta=0.50$ soit à haute température) tous les autres exhibent des divergences concernant principalement les coefficients $C_{01}$. Même si on n'exclut pas un bug sous-jacent, les distributions de ces coefficients générés montrent tout de même des structures sauf précisément pour $\phi_4$, $\beta=0.50$.

Deux expérimentations supplémentaires ont été menéee pour essayer de comprendre la distribution des $C_{01}$:
\begin{itemize}
\itemb la première concerne les données WL-1 mais avec les cartes à l'échelle $L=64$. Le résultat est présenté sur la figure \ref{fig-WL1-64x64-summary-stat}. Les désaccords des distributions $C_{01}$ sont présents. Notons que les autres distributions sont très bien reproduites. 
\begin{figure}
\centering
\includegraphics[width=0.99\textwidth]{fig-WL1-64x64-summary-stat.png}
\caption{Statistiques de la figure \ref{fig-WL1-summary-stat} mais concernant les données WL-1 à l'échelle $L=64$. Notons que l'on dispose de 500 cartes "train" et de 50 cartes "wcrg".}
\label{fig-WL1-64x64-summary-stat}
\end{figure}
\itemb la seconde reprend la synthèse avec un modèle micro-canonique mais où l'on a volontairement fixé à 0 les coefficients $C_{01}$. Le résultat est illustré sur la figure \ref{fig-WL1-micro-C01scaling}.
\begin{figure}
\centering
\includegraphics[width=0.99\textwidth]{fig-WL1-micro-C01scaling.png}
\caption{Statistiques de la figure \ref{fig-WL1-summary-stat} mais concernant les données WL-1 et un modèle micro-canonique du type de celui discuté à la section \ref{sec-WL1-res} mais où les coefficients $C_{01}$ sont volontairement mis à 0. Notons que l'on a utilisé qu'une unique carte pour calculer les distributions.}
\label{fig-WL1-micro-C01scaling}
\end{figure}
Au-delà de la distribution des $C_{01}$, l'hypothèse d'une corrélation des désaccords $C_{01}$,  bi-spectrum et $s_{21}$ peut sembler avoir un certain sens.
\end{itemize}
%
\section{Discussion}\label{sec-discussion}
%
Nous allons tenter de donner des éléments afin de comprendre le comportement des statistiques $C_{01}$ et $C_{11}$.

\paragraph*{$C_{01}$ et $C_{11}$ dans un cadre gaussien}
Pour un processus $x$ gaussien, les statistiques $C_{01}$ et $C_{11}$ sont contraintes par le spectre de puissance.

$C_{11}$ correspond à l'expression de la matrice de covariance $\langle|x*\psi_{j_1}|(u)|x*\psi_{j_2}|^\ast(u+\tau)\rangle_{\tau}$ dans une base qui la quasi-diagonalise \cite{morel2023scale}. Dans le cas gaussien, si $\hat\psi_{j_1}\hat\psi_{j_2} = 0$, $x*\psi_{j_1}$ et $x*\psi_{j_2}$ sont indépendantes \cite{morel2023scale}, et  
\begin{align*}
\langle|x*\psi_{j_1}||x*\psi_{j_2}|\rangle &= \mathbb{E}[|x*\psi_{j_1}|]\mathbb{E}[|x*\psi_{j_2}|] \\
& = \text{Cste}\sqrt{\mathbb{E}[|x*\psi_{j_1}|^2]}^{\frac{1}{2}}\sqrt{\mathbb{E}[|x*\psi_{j_2}|^2]}^{\frac{1}{2}}.
\end{align*}
Ainsi, $C_{11}$ est contraint par le spectre de puissance (\textit{nb. en physique le spectre de puissance fait plutôt référence au filtrage avec les exponentielles complexes à la place des ondelettes.}). 

Le même argument donne que $C_{01}$ vaut $0$. En effet, 
$$
\langle |(x*\psi_{j_1}|*\psi_{j_2})(x*\psi_{j_2})\rangle = \mathbb{E}[|x*\psi_{j_1}|*\psi_{j_2}]\mathbb{E}[x*\psi_{j_2}] = 0
$$. 
On peut donc s'attendre, en l'absence de contraintes non-gaussiennes, à ce que $C_{01}$ soit nul.

Par ailleurs, pour le WCRG, les déviations non-gaussiennes de $C_{01}$ et de $C_{11}$ sont une conséquence du potentiel non-linéaire qui s'applique sur $x_{j-1}$ conditionnellement à $x_j$, et donc conditionnellement à toutes les bandes fréquences inférieures à celle de $\bar x_j$.

\paragraph*{Non Gaussianité imposée par le potentiel scalaire}

La partie non-gausienne des potentiels est faite de potentiels scalaires non linéaires pixel par pixel sur les cartes $x_j$. Les coefficients des cartes $\bar x_{j+1}$ sont mélangés non-linéairement conditionnellement à la basse fréquence $x_{j+1}$. Les potentiels scalaires imposent des conditions d'amplitude sur $x_j$, sans conditions spatiales.  Cela impose par conséquent une condition d'amplitude sur $\bar x_{j+1}$, sans cohérence spatiale (autre que le conditionnement par la basse fréquence). 

On peut extrapoler et conclure à une condition d'amplitude sur les cartes $x\ast\psi_j$, sans condition de phase, qui correspond à la cohérence spatiale.  En effet, si on imagine que $\bar x_j$ est une sinusoïde, contraindre uniquement sa distribution ne contraint pas sa phase relative. On contraindrait alors l'amplitude de $x*\psi_j$, sans contraindre sa phase.

Or, $C_{11}$ est une mesure d'amplitude, et non pas de phase. La contrainte d'amplitude, conséquence du potentiel non-linéaire, peut alors permettre la bonne reproduction de $C_{11}$. Au contraire, $C_{01}$ est une statistique de phases. Comme ces dernières ne sont pas contraintes, les synthèses ne devraient donc pas exhiber de caractère non-gaussien sur $C_{01}$.

%{En particulier, même si les ondelettes ne sont pas symétriques, le mélange des voisinages à chaque échelle est toujours le même, ne laissant pas la place à un équilibre symétrie/asymétrie.
%Il est possible que le moment d'ordre 4 bien reproduit soit une conséquence du moment d'ordre 2 bien reproduit.}

\paragraph*{Concordance expérimentale}

On peut discuter séparément le cas concernant le système $\varphi^4$ de celui des données de Weak Lensing.
\begin{itemize}
\item[- $\varphi^4$:]
Dans le cas à haute température ($\beta = 0.5$), les statistiques $C_{01}$, qui sont attendues d'être nulles, sont bien reproduites. Pour ce modèle, on perd la corrélation entre échelles, et par conséquent il s'agit d'un modèle quasiment marginal, bien reproduit par les potentiels scalaires. 

Au point critique ($\beta= 0.68$), la non-gaussianité très forte (car toutes les échelles sont corrélées) de $C_{01}$ n'est pas capturée, et les statistiques reproduites sont nulles. Par contre,  $C_{11}$ est bien reproduite.

Enfin, A basse température ($\beta = 0.76$), la corrélation entre échelles est plus faible mais non nulle, la non-gaussianité est sous évaluée par $C_{01}$, qui est plus petit sur les synthèses que sur les données.

\item[- Weak Lensing:]
Pour WL2, $C_{01}$ souffre de sous-évaluation, comme dans le cas précédent $\varphi^4$  $\beta = 0.76$.  Concernant WL1, $C_{01}$ est quasiment nul, sans structuration, contrairement aux cartes d'entraînement. Il serait envisageable de refaire un entraînement sans procéder à la gaussianisation des pixels pour lever un doute sur d'entraînement. Dans les deux cas de Weak Lensing, les $C_{11}$ sont bien reproduits.
\end{itemize}

%On constate numériquement que le modèle WCRG peine à reproduire toute l'asymétrie de la distribution sous-jacente $p(x)$.
\paragraph*{Conséquence sur la physique apprise}

\begin{figure}
\centering
\includegraphics[width=0.70\textwidth]{fig_WL_C01-change-micro.png}\\
\caption{Impact des statistiques $C_{01}$ sur des synthèses micro-canoniques.}
\label{fig-WL-C01-micro_change}
\end{figure}

On peut se poser la question de savoir ce que cela implique sur la physique. On argumente qu'augmenter $C_{01}$ concentrerait le champ autour de pics et inversement les mettre à 0, diminue les pics. 

On voit figure \ref{fig-WL-C01-micro_change} qu'augmenter le coefficient $C_{01}$, à spectre de puissance fixé, favorise le clustering à petite échelle spatiale, et donc symétriquement le modèle WCRG tel qu'il est actuellement défini a tendance à diluer les corrélations à courte portée. 

$C_{01}$ est proportionnel à  $\langle(x \ast \psi_2)(|x \ast \psi_1| \ast \psi_2)^\ast\rangle$.
A spectre de puissance fixé, si ce coefficient est grand, c'est que les champs $x \ast \psi_2$ et $|x\ast \psi_1| \ast \psi_2$ ont  leurs énergies qui sont  non seulement spatialement localisées aux mêmes endroits, mais avec une phase relative nulle. On s'attend à des interférences constructives en ces points, et donc, plus ces stats sont grandes, plus les pics seront forts. Au contraire quand $C_{01}$ est nul, les phases des différentes échelles ne s'alignent pas et ne se combinent pas positivement. 

%
\section{Résumé}
% 
Dans cette note, nous avons exposé les résultats de calculs de statistiques permettant d'aller au-delà de l'aspect visuel des cartes générées et le matching des spectres de puissance, afin d'apprécier la qualité de synthèse des modèles WCRG dans le cas de données de Weak Lensing et $\phi_4$. \textbf{Ce qui en ressort à l'heure de l'écriture de cette note, c'est que si globalement les distributions d'ordre supérieur sont bien reproduites, ce qui est un résultat nouveau très encourageant, il en reste une qui fait défaut à savoir celle des coefficients de corrélation $C_{01}$}.  Le cas particulier où $C_{01}$ est bien reproduit concerne le système $\phi_4$ avec $\beta=0.50$ (haute température), mais il se trouve que dans ce cas précis on s'attend à ce que $C_{01}$ soit nul.

Notons qu'un réseau DGAN optimisé sur les données WL-1 donne des cartes dont les statistiques sont en parfait accord avec celles obtenues sur les données d'entraînement. Il en va de même avec des modèles micro-canoniques mais qui par nature sont optimisés de telles distributions.

Donc, il "reste" à comprendre comment on peut améliorer la modélisation des potentiels WCRG afin d'obtenir de meilleurs résultats. Une piste a été élaborée dans la discussion de la section \ref{sec-discussion}: elle concernerait les phases relatives des différentes bandes fréquences, qui ne seraient pas contraintes par le modèle.

%%%%%%%%%%
%\newpage
\addcontentsline{toc}{section}{Références}
% Put your bibiliography file here
%\section{Bibliography}
\bibliographystyle{aa}
\bibliography{refs.bib}

\appendix
\section{Plots de contrôle...}
Les histogrammes de cette section sont issues de la phase de synthèse du modèle WCRG entraîner sur les données WL-1 (Sec.~\ref{sec-wcrg-WL1-synt}).
\begin{figure}[h]
\centering
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L1-pixelval.png}
\caption{Comparaison des valeurs de pixels à l'échelle $L=1$.}
\label{fig-WL1-synt-L1-pixelval}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L2-pixelval.png}\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L2-details_1.png}\\
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L2-details_2.png}
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L2-details_3.png}
\caption{Comparaison à l'échelle $L=2$ entre cartes de type "originale/entraînement" et de type "synthétisée": distribution des valeurs de pixels de cartes "totales" (haut-gauche), de détails "H" (haut-droite), de détails "V" (bas-gauche) et de détails "D" (bas-droite). }
\label{fig-WL1-synt-L2}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L4-pixelval.png}\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L4-details_1.png}\\
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L4-details_2.png}
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L4-details_3.png}
\caption{Echelle $L=4$: légende identique à la figure \ref{fig-WL1-synt-L2}.}
\label{fig-WL1-synt-L4}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L8-pixelval.png}\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L8-details_1.png}\\
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L8-details_2.png}
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L8-details_3.png}
\caption{Echelle $L=8$: légende identique à la figure \ref{fig-WL1-synt-L2}.}
\label{fig-WL1-synt-L8}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L16-pixelval.png}\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L16-details_1.png}\\
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L16-details_2.png}
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L16-details_3.png}
\caption{Echelle $L=16$: légende identique à la figure \ref{fig-WL1-synt-L2}.}
\label{fig-WL1-synt-L16}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L32-pixelval.png}\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L32-details_1.png}\\
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L32-details_2.png}
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L32-details_3.png}
\caption{Echelle $L=32$: légende identique à la figure \ref{fig-WL1-synt-L2}.}
\label{fig-WL1-synt-L32}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L64-pixelval.png}\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L64-details_1.png}\\
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L64-details_2.png}
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L64-details_3.png}
\caption{Echelle $L=64$: légende identique à la figure \ref{fig-WL1-synt-L2}.}
\label{fig-WL1-synt-L64}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L128-pixelval.png}\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L128-details_1.png}\\
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L128-details_2.png}
\includegraphics[width=0.35\textwidth]{fig-WL1-synt-L128-details_3.png}
\caption{Echelle $L=128$ (la plus grande possible): légende identique à la figure \ref{fig-WL1-synt-L2}.}
\label{fig-WL1-synt-L128}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
